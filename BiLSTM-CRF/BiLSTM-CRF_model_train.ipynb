{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be1f5a4a",
   "metadata": {},
   "source": [
    "ref: https://github.com/SuphanutN/Thai-NER-BiLSTMCRF-WordCharEmbedding/blob/master/Thai_NER_WordCharacterEmbedding_Train.ipynb\n",
    "\n",
    "mycolab: https://colab.research.google.com/drive/1KF_w6wSNSfCQqPEzrJK7NDVJ2uWiKMA2?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ea0dd",
   "metadata": {},
   "source": [
    "# Install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec02ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras==2.1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d4568",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c9937",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ca6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install XlsxWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c451718",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad66c074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12c988ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/crimex/miniconda3/lib/python3.10/site-packages/keras/backend/tensorflow_backend.py:2871: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if training is 1 or training is True:\n",
      "/home/crimex/miniconda3/lib/python3.10/site-packages/keras/backend/tensorflow_backend.py:2877: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif training is 0 or training is False:\n",
      "2023-05-19 21:34:33.605511: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-19 21:34:34.086276: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-19 21:34:34.086323: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-19 21:34:34.086327: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "# Save / Load File\n",
    "import dill\n",
    "import pickle\n",
    "\n",
    "# Plot Graph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sklearn Report\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import chain\n",
    "\n",
    "# Load Vectors\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Utility\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Model Utility\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# keras\n",
    "# Navigate to miniconda3/lib/python3.10/site-packages/keras/callbacks.py:18\n",
    "# Add .abc after collection line: 18 - from collections.abc import Iterable\n",
    "# https://www.nbshare.io/notebook/589110167/ImportError-cannot-import-name-Iterabler-from-collections-python-3-10/\n",
    "import keras\n",
    "# Keras Model\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model, Input\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D\n",
    "from keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# other \n",
    "import tensorflow as tf\n",
    "# tf.disable_v2_behavior()\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "351724db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb25735",
   "metadata": {},
   "source": [
    "## Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e021232",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='/home/crimex/CRIMEX/BiLSTM-CRF/'\n",
    "# RAW_PATH = DATA_PATH +'data/'\n",
    "# MODEL_PATH = DATA_PATH +'model/Keras/WordCharModel/'\n",
    "W_MODEL_PATH = DATA_PATH +'model/thai2fit/'\n",
    "Dict_MODEL_PATH = DATA_PATH +'model/dictionary/'\n",
    "\n",
    "# dataset_name = 'bilstm_train3.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a12854be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_PATH           :  /home/crimex/CRIMEX/BiLSTM-CRF/model/Keras/WordCharModel/sent/final/\n",
      "\n",
      "Train file           :  /home/crimex/CRIMEX/Dataset/crf/sent/train_final\n",
      "Test file            :  /home/crimex/CRIMEX/Dataset/crf/sent/test_final\n",
      "\n",
      "txt eval result      :  /home/crimex/CRIMEX/BiLSTM-CRF/report_sent_final.txt\n",
      "xls predicted result :  /home/crimex/CRIMEX/predicted_bert/final_sent_bilstm.xlsx\n"
     ]
    }
   ],
   "source": [
    "typ = 'sent'\n",
    "path = 'final'\n",
    "di = '/home/crimex/CRIMEX/Dataset/crf/'+ typ+'/'\n",
    "train_path = di + 'train_'+ path\n",
    "test_path = di + 'test_' + path \n",
    "save_result_xls = '/home/crimex/CRIMEX/predicted_bert/' + path+ '_'+ typ+'_bilstm.xlsx'\n",
    "MODEL_PATH = '/home/crimex/CRIMEX/BiLSTM-CRF/model/Keras/WordCharModel/'+typ +'/' +path +'/'\n",
    "save_txt = '/home/crimex/CRIMEX/BiLSTM-CRF/report_' + typ+ '_'+path +'.txt'\n",
    "\n",
    "# print(\"directory path       : \", di)\n",
    "print(\"MODEL_PATH           : \", MODEL_PATH)\n",
    "print(\"\\nTrain file           : \", train_path)\n",
    "print(\"Test file            : \", test_path)\n",
    "print(\"\\ntxt eval result      : \", save_txt)\n",
    "print(\"xls predicted result : \", save_result_xls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c9a69b",
   "metadata": {},
   "source": [
    "# Load raw dataset (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9769c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_path+'.txt', \"r\") as f:\n",
    "    train = [line.strip() for line in f.readlines()]\n",
    "    f.close()\n",
    "    \n",
    "with open(test_path+'.txt', \"r\") as f:\n",
    "    test = [line.strip() for line in f.readlines()]\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdb5b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "first = True\n",
    "datatofile = []\n",
    "train_sents = []\n",
    "test_sents = []\n",
    "\n",
    "for tra in train:\n",
    "    sent = []\n",
    "    for itm in ast.literal_eval(tra):\n",
    "        new_itm = itm[:1] + itm[2:]\n",
    "        sent.append(new_itm)\n",
    "    datatofile.append(list(sent))\n",
    "    train_sents.append(list(sent))\n",
    "\n",
    "for tes in test:\n",
    "    sent = []\n",
    "    for itm in ast.literal_eval(tes):\n",
    "        new_itm = itm[:1] + itm[2:]\n",
    "        sent.append(new_itm)\n",
    "    datatofile.append(list(sent))\n",
    "    test_sents.append(list(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0416f0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55410\n",
      "9336\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sents))\n",
    "print(len(test_sents))\n",
    "# print(train_sents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31c110",
   "metadata": {},
   "source": [
    "## Load Thai2Fit Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aa7d23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/crimex/CRIMEX/BiLSTM-CRF/model/thai2fit/thai2vecNoSym.bin\n"
     ]
    }
   ],
   "source": [
    "# download file from https://github.com/SuphanutN/Thai-NER-BiLSTMCRF-WordCharEmbedding/blob/master/model/thai2fit/thai2vecNoSym.bin\n",
    "thai2fit_model = KeyedVectors.load_word2vec_format(W_MODEL_PATH+'thai2vecNoSym.bin',binary=True)\n",
    "thai2fit_weight = thai2fit_model.vectors\n",
    "print(W_MODEL_PATH+'thai2vecNoSym.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5ddafb",
   "metadata": {},
   "source": [
    "## Preprocess Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f12db8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20487\n",
      "22\n",
      "55677\n",
      "{'B-Action': 0, 'B-Criminal': 1, 'B-Datetime': 2, 'B-Enforcement': 3, 'B-Item': 4, 'B-Location': 5, 'B-Rootcause': 6, 'B-Trigger': 7, 'B-Victim': 8, 'B-worth': 9, 'I-Action': 10, 'I-Criminal': 11, 'I-Datetime': 12, 'I-Enforcement': 13, 'I-Item': 14, 'I-Location': 15, 'I-Rootcause': 16, 'I-Trigger': 17, 'I-Victim': 18, 'I-worth': 19, 'O': 20, 'pad': 21}\n"
     ]
    }
   ],
   "source": [
    "word_list=[]\n",
    "ner_list=[]\n",
    "thai2dict = {}\n",
    "\n",
    "for sent in train_sents:\n",
    "    for word in sent:\n",
    "        word_list.append(word[0])\n",
    "        ner_list.append(word[1])\n",
    "        \n",
    "for word in thai2fit_model.index_to_key:\n",
    "    thai2dict[word] = thai2fit_model[word]\n",
    "\n",
    "word_list.append(\"pad\")\n",
    "word_list.append(\"unknown\") #Special Token for Unknown words (\"UNK\")\n",
    "ner_list.append(\"pad\")\n",
    "\n",
    "all_words = sorted(set(word_list))\n",
    "all_ner = sorted(set(ner_list))\n",
    "all_thai2dict = sorted(set(thai2dict))\n",
    "\n",
    "word_to_ix = dict((c, i) for i, c in enumerate(all_words)) #convert word to index \n",
    "ner_to_ix = dict((c, i) for i, c in enumerate(all_ner)) #convert ner to index\n",
    "thai2dict_to_ix = dict((c, i) for i, c in enumerate(thai2dict)) #convert thai2fit to index \n",
    "\n",
    "ix_to_word = dict((v,k) for k,v in word_to_ix.items()) #convert index to word\n",
    "ix_to_ner = dict((v,k) for k,v in ner_to_ix.items())  #convert index to ner\n",
    "ix_to_thai2dict = dict((v,k) for k,v in thai2dict_to_ix.items())  #convert index to thai2fit\n",
    "\n",
    "n_word = len(word_to_ix)\n",
    "n_tag = len(ner_to_ix)\n",
    "n_thai2dict = len(thai2dict_to_ix)\n",
    "print(n_word)\n",
    "print(n_tag)\n",
    "print(n_thai2dict)\n",
    "print(ner_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d05731",
   "metadata": {},
   "source": [
    "## Preprocess Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ec3ccb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399\n"
     ]
    }
   ],
   "source": [
    "chars = set([w_i for w in thai2dict for w_i in w])\n",
    "char2idx = {c: i + 5 for i, c in enumerate(chars)}\n",
    "\n",
    "char2idx[\"pad\"] = 0\n",
    "char2idx[\"unknown\"] = 1\n",
    "char2idx[\" \"] = 2\n",
    "\n",
    "char2idx[\"$\"] = 3\n",
    "char2idx[\"#\"] = 4\n",
    "char2idx[\"!\"] = 5\n",
    "char2idx[\"%\"] = 6\n",
    "char2idx[\"&\"] = 7\n",
    "char2idx[\"*\"] = 8\n",
    "char2idx[\"+\"] = 9\n",
    "char2idx[\",\"] = 10\n",
    "char2idx[\"-\"] = 11\n",
    "char2idx[\".\"] = 12\n",
    "char2idx[\"/\"] = 13\n",
    "char2idx[\":\"] = 14\n",
    "char2idx[\";\"] = 15\n",
    "char2idx[\"?\"] = 16\n",
    "char2idx[\"@\"] = 17\n",
    "char2idx[\"^\"] = 18\n",
    "char2idx[\"_\"] = 19\n",
    "char2idx[\"`\"] = 20\n",
    "char2idx[\"=\"] = 21\n",
    "char2idx[\"|\"] = 22\n",
    "char2idx[\"~\"] = 23\n",
    "char2idx[\"'\"] = 24\n",
    "char2idx['\"'] = 25\n",
    "\n",
    "char2idx[\"(\"] = 26\n",
    "char2idx[\")\"] = 27\n",
    "char2idx[\"{\"] = 28\n",
    "char2idx[\"}\"] = 29\n",
    "char2idx[\"<\"] = 30\n",
    "char2idx[\">\"] = 31\n",
    "char2idx[\"[\"] = 32\n",
    "char2idx[\"]\"] = 33\n",
    "\n",
    "n_chars = len(char2idx)\n",
    "print(n_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dfa953",
   "metadata": {},
   "source": [
    "## Save Dictionary for Character and NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0b05f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/crimex/CRIMEX/BiLSTM-CRF/model/dictionary/chardict.pickle\n"
     ]
    }
   ],
   "source": [
    "print(Dict_MODEL_PATH+'chardict.pickle')\n",
    "with open(Dict_MODEL_PATH+'chardict.pickle', 'wb') as chardict: #wb\n",
    "    pickle.dump(char2idx, chardict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a0a0898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/crimex/CRIMEX/BiLSTM-CRF/model/dictionary/nerdict.pickle\n"
     ]
    }
   ],
   "source": [
    "print(Dict_MODEL_PATH+'nerdict.pickle')\n",
    "with open(Dict_MODEL_PATH+'nerdict.pickle', 'wb') as nerdict: #wb\n",
    "    pickle.dump(ner_to_ix, nerdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7275d4eb",
   "metadata": {},
   "source": [
    "# Set Parameter and Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ade7f3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 250\n",
    "max_len_char = 30\n",
    "\n",
    "character_LSTM_unit = 32\n",
    "char_embedding_dim = 32\n",
    "main_lstm_unit = 256 ## Bidirectional 256 + 256 = 512\n",
    "lstm_recurrent_dropout = 0.5\n",
    "\n",
    "train_batch_size = 32\n",
    "train_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a374485",
   "metadata": {},
   "source": [
    "## Mapping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "579305d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_word(input_text):\n",
    "    idxs = list()\n",
    "    for word in input_text:\n",
    "        if word in thai2dict:\n",
    "            idxs.append(thai2dict_to_ix[word])\n",
    "        else:\n",
    "            idxs.append(thai2dict_to_ix[\"unknown\"]) #Use UNK tag for unknown word\n",
    "    return idxs\n",
    "\n",
    "def prepare_sequence_target(input_label):\n",
    "    idxs = [ner_to_ix[w] for w in input_label]\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3bbb7d",
   "metadata": {},
   "source": [
    "### Split word and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c78dc798",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sent =[ [ word[0] for word in sent]for sent in train_sents ] #words only\n",
    "train_targets =[ [ word[1] for word in sent]for sent in train_sents ] #NER only\n",
    "\n",
    "input_test_sent =[ [ word[0] for word in sent]for sent in test_sents ] #words only\n",
    "test_targets =[ [ word[1] for word in sent]for sent in test_sents ] #NER only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5bd106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_test_sent[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4462c9b2",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec510aa",
   "metadata": {},
   "source": [
    "## Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1bbbf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word Training\n",
    "X_word_tr = [prepare_sequence_word(s) for s in input_sent]\n",
    "X_word_tr = pad_sequences(maxlen=max_len, sequences=X_word_tr, value=thai2dict_to_ix[\"pad\"], padding='post', truncating='post')\n",
    "\n",
    "## Character Training\n",
    "X_char_tr = []\n",
    "for sentence in train_sents:\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                if(sentence[i][0][j] in char2idx):\n",
    "                    word_seq.append(char2idx.get(sentence[i][0][j]))\n",
    "                else:\n",
    "                    word_seq.append(char2idx.get(\"unknown\"))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"pad\"))\n",
    "        sent_seq.append(word_seq)\n",
    "    X_char_tr.append(np.array(sent_seq))\n",
    "\n",
    "## Sequence Label Training\n",
    "y_tr = [prepare_sequence_target(s) for s in train_targets]\n",
    "y_tr = pad_sequences(maxlen=max_len, sequences=y_tr, value=ner_to_ix[\"pad\"], padding='post', truncating='post')\n",
    "y_tr = [to_categorical(i, num_classes=n_tag) for i in y_tr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beabe2f",
   "metadata": {},
   "source": [
    "## Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4944167",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word Testing\n",
    "X_word_te = [prepare_sequence_word(s) for s in input_test_sent]\n",
    "X_word_te = pad_sequences(maxlen=max_len, sequences=X_word_te, value=thai2dict_to_ix[\"pad\"], padding='post', truncating='post')\n",
    "\n",
    "## Character Testing\n",
    "X_char_te = []\n",
    "for sentence in test_sents:\n",
    "    sent_seq = []\n",
    "    for i in range(max_len):\n",
    "        word_seq = []\n",
    "        for j in range(max_len_char):\n",
    "            try:\n",
    "                if(sentence[i][0][j] in char2idx):\n",
    "                    word_seq.append(char2idx.get(sentence[i][0][j]))\n",
    "                else:\n",
    "                    word_seq.append(char2idx.get(\"unknown\"))\n",
    "            except:\n",
    "                word_seq.append(char2idx.get(\"pad\"))    \n",
    "        sent_seq.append(word_seq)\n",
    "    X_char_te.append(np.array(sent_seq))\n",
    "\n",
    "## Sequence Label Testing\n",
    "y_te = [prepare_sequence_target(s) for s in test_targets]\n",
    "y_te = pad_sequences(maxlen=max_len, sequences=y_te, value=ner_to_ix[\"pad\"], padding='post', truncating='post')\n",
    "y_te = [to_categorical(i, num_classes=n_tag) for i in y_te]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f23c90",
   "metadata": {},
   "source": [
    "# Initial Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e010bcbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 21:37:12.752006: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/crimex/miniconda3/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 21:37:13.039333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 21:37:13.039611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 21:37:13.066599: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-05-19 21:37:13.066621: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-05-19 21:37:13.068740: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2023-05-19 21:37:13.075839: W tensorflow/c/c_api.cc:291] Operation '{name:'word_embedding/embeddings/Assign' id:12 op device:{requested: '', assigned: ''} def:{{{node word_embedding/embeddings/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](word_embedding/embeddings, word_embedding/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-05-19 21:37:13.216280: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 21:37:13.216575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-19 21:37:13.216777: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, 250, 30)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_input_ (InputLayer)        (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 250, 30, 32)  12768       char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 250, 400)     22270800    word_input_[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 250, 64)      16640       time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 250, 464)     0           word_embedding[0][0]             \n",
      "                                                                 time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 250, 464)     0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 250, 512)     1476608     spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 250, 50)      25650       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, 250, 22)      1650        time_distributed_3[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 23,804,116\n",
      "Trainable params: 1,533,316\n",
      "Non-trainable params: 22,270,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crimex/miniconda3/lib/python3.10/site-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "/home/crimex/miniconda3/lib/python3.10/site-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Word Input\n",
    "# -Navigating to, \"~/miniconda3/lib/python3.10/site-packages/keras/backend/tensorflow_backend.py\".\n",
    "# Then, replace tensorflow import with i\n",
    "\n",
    "# import tensorflow.compat.v1 as tf\n",
    "# tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Now, on line 425 of tensorflow_backend.py: change the hyperparameter dtype from None to tf.float32\n",
    "\n",
    "# ref: https://github.com/matterport/Mask_RCNN/issues/2075 (Nishanth2708 commented on Aug 26, 2021)\n",
    "word_in = Input(shape=(max_len,), name='word_input_')\n",
    "\n",
    "# Word Embedding Using Thai2Fit\n",
    "word_embeddings = Embedding(input_dim=n_thai2dict,\n",
    "                            output_dim=400,\n",
    "                            weights = [thai2fit_weight],input_length=max_len,\n",
    "                            mask_zero=False,\n",
    "                            name='word_embedding', trainable=False)(word_in)\n",
    "\n",
    "# Character Input\n",
    "char_in = Input(shape=(max_len, max_len_char,), name='char_input')\n",
    "\n",
    "# Character Embedding\n",
    "emb_char = TimeDistributed(Embedding(input_dim=n_chars, output_dim=char_embedding_dim, \n",
    "                           input_length=max_len_char, mask_zero=False))(char_in)\n",
    "\n",
    "# Character Sequence to Vector via BiLSTM\n",
    "char_enc = TimeDistributed(Bidirectional(LSTM(units=character_LSTM_unit, return_sequences=False, recurrent_dropout=lstm_recurrent_dropout)))(emb_char)\n",
    "\n",
    "\n",
    "# Concatenate All Embedding\n",
    "all_word_embeddings = concatenate([word_embeddings, char_enc])\n",
    "all_word_embeddings = SpatialDropout1D(0.3)(all_word_embeddings)\n",
    "\n",
    "# Main Model BiLSTM\n",
    "main_lstm = Bidirectional(LSTM(units=main_lstm_unit, return_sequences=True,\n",
    "                               recurrent_dropout=lstm_recurrent_dropout))(all_word_embeddings)\n",
    "main_lstm = TimeDistributed(Dense(50, activation=\"relu\"))(main_lstm)\n",
    "\n",
    "# CRF\n",
    "crf = CRF(n_tag)  # CRF layer\n",
    "out = crf(main_lstm)  # output\n",
    "\n",
    "# Model\n",
    "model = Model([word_in, char_in], out)\n",
    "\n",
    "# -Navigating to, \"~/miniconda3/lib/python3.10/site-packages/keras/optimizers.py\".\n",
    "\n",
    "# add compat.v1. after tf. (line: 757)\n",
    "\n",
    "# ref: https://itecnote.com/tecnote/python-error-importing-bert-module-tensorflow-_api-v2-train-has-no-attribute-optimizer/\n",
    "model.compile(optimizer=\"adam\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434096d5",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88cdeb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/crimex/CRIMEX/BiLSTM-CRF/model/Keras/WordCharModel/sent/final/\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3368fae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55410 samples, validate on 9336 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-19 21:37:16.952667: W tensorflow/c/c_api.cc:291] Operation '{name:'training/Adam/Variable_59/Assign' id:3882 op device:{requested: '', assigned: ''} def:{{{node training/Adam/Variable_59/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training/Adam/Variable_59, training/Adam/zeros_59)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55410/55410 [==============================] - 1537s 28ms/step - loss: 0.0394 - crf_viterbi_accuracy: 0.9869 - val_loss: 0.0131 - val_crf_viterbi_accuracy: 0.9892\n",
      "\n",
      "Epoch 00001: val_crf_viterbi_accuracy improved from -inf to 0.98918, saving model to /home/crimex/CRIMEX/BiLSTM-CRF/model/Keras/WordCharModel/sent/final/weights-improvement-01-0.989.hdf5\n",
      "Epoch 2/20\n",
      "55410/55410 [==============================] - 1537s 28ms/step - loss: 0.0038 - crf_viterbi_accuracy: 0.9904 - val_loss: -0.0016 - val_crf_viterbi_accuracy: 0.9904\n",
      "\n",
      "Epoch 00002: val_crf_viterbi_accuracy improved from 0.98918 to 0.99044, saving model to /home/crimex/CRIMEX/BiLSTM-CRF/model/Keras/WordCharModel/sent/final/weights-improvement-02-0.990.hdf5\n",
      "Epoch 3/20\n",
      "55410/55410 [==============================] - 1533s 28ms/step - loss: -0.0075 - crf_viterbi_accuracy: 0.9911 - val_loss: -0.0109 - val_crf_viterbi_accuracy: 0.9902\n",
      "\n",
      "Epoch 00003: val_crf_viterbi_accuracy did not improve from 0.99044\n",
      "Epoch 4/20\n",
      "55410/55410 [==============================] - 1530s 28ms/step - loss: -0.0145 - crf_viterbi_accuracy: 0.9913 - val_loss: -1.1253e-04 - val_crf_viterbi_accuracy: 0.9908\n",
      "\n",
      "Epoch 00004: val_crf_viterbi_accuracy improved from 0.99044 to 0.99075, saving model to /home/crimex/CRIMEX/BiLSTM-CRF/model/Keras/WordCharModel/sent/final/weights-improvement-04-0.991.hdf5\n",
      "Epoch 5/20\n",
      "55410/55410 [==============================] - 1529s 28ms/step - loss: -0.0102 - crf_viterbi_accuracy: 0.9909 - val_loss: 0.0242 - val_crf_viterbi_accuracy: 0.9903\n",
      "\n",
      "Epoch 00005: val_crf_viterbi_accuracy did not improve from 0.99075\n",
      "Epoch 6/20\n",
      "55410/55410 [==============================] - 1512s 27ms/step - loss: -0.0391 - crf_viterbi_accuracy: 0.9906 - val_loss: -0.1016 - val_crf_viterbi_accuracy: 0.9909\n",
      "\n",
      "Epoch 00006: val_crf_viterbi_accuracy improved from 0.99075 to 0.99092, saving model to /home/crimex/CRIMEX/BiLSTM-CRF/model/Keras/WordCharModel/sent/final/weights-improvement-06-0.991.hdf5\n",
      "Epoch 7/20\n",
      "55410/55410 [==============================] - 1516s 27ms/step - loss: -0.1125 - crf_viterbi_accuracy: 0.9904 - val_loss: 0.2566 - val_crf_viterbi_accuracy: 0.9905\n",
      "\n",
      "Epoch 00007: val_crf_viterbi_accuracy did not improve from 0.99092\n",
      "Epoch 8/20\n",
      "55410/55410 [==============================] - 1519s 27ms/step - loss: -0.2931 - crf_viterbi_accuracy: 0.9903 - val_loss: -1.0661 - val_crf_viterbi_accuracy: 0.9787\n",
      "\n",
      "Epoch 00008: val_crf_viterbi_accuracy did not improve from 0.99092\n",
      "Epoch 9/20\n",
      "55410/55410 [==============================] - 1509s 27ms/step - loss: -0.5926 - crf_viterbi_accuracy: 0.9899 - val_loss: 1.4883 - val_crf_viterbi_accuracy: 0.9897\n",
      "\n",
      "Epoch 00009: val_crf_viterbi_accuracy did not improve from 0.99092\n",
      "Epoch 10/20\n",
      "55410/55410 [==============================] - 1503s 27ms/step - loss: -1.2770 - crf_viterbi_accuracy: 0.9902 - val_loss: -2.9987 - val_crf_viterbi_accuracy: 0.9902\n",
      "\n",
      "Epoch 00010: val_crf_viterbi_accuracy did not improve from 0.99092\n",
      "Epoch 11/20\n",
      "55410/55410 [==============================] - 1502s 27ms/step - loss: -1.9802 - crf_viterbi_accuracy: 0.9902 - val_loss: -3.5429 - val_crf_viterbi_accuracy: 0.9901\n",
      "\n",
      "Epoch 00011: val_crf_viterbi_accuracy did not improve from 0.99092\n",
      "Epoch 12/20\n",
      "55410/55410 [==============================] - 1502s 27ms/step - loss: -2.8724 - crf_viterbi_accuracy: 0.9900 - val_loss: 2.3883 - val_crf_viterbi_accuracy: 0.9893\n",
      "\n",
      "Epoch 00012: val_crf_viterbi_accuracy did not improve from 0.99092\n",
      "Epoch 13/20\n",
      "55410/55410 [==============================] - 1503s 27ms/step - loss: -3.6898 - crf_viterbi_accuracy: 0.9901 - val_loss: -0.1980 - val_crf_viterbi_accuracy: 0.9899\n",
      "\n",
      "Epoch 00013: val_crf_viterbi_accuracy did not improve from 0.99092\n",
      "Epoch 14/20\n",
      "55410/55410 [==============================] - 1502s 27ms/step - loss: -5.1179 - crf_viterbi_accuracy: 0.9903 - val_loss: 1.4891 - val_crf_viterbi_accuracy: 0.9903\n",
      "\n",
      "Epoch 00014: val_crf_viterbi_accuracy did not improve from 0.99092\n",
      "Epoch 15/20\n",
      "55410/55410 [==============================] - 1502s 27ms/step - loss: -5.4788 - crf_viterbi_accuracy: 0.9901 - val_loss: 0.2100 - val_crf_viterbi_accuracy: 0.9895\n",
      "\n",
      "Epoch 00015: val_crf_viterbi_accuracy did not improve from 0.99092\n",
      "Epoch 16/20\n",
      "55410/55410 [==============================] - 1502s 27ms/step - loss: -6.7731 - crf_viterbi_accuracy: 0.9899 - val_loss: 8.2963 - val_crf_viterbi_accuracy: 0.9892\n",
      "\n",
      "Epoch 00016: val_crf_viterbi_accuracy did not improve from 0.99092\n",
      "Epoch 17/20\n",
      "55410/55410 [==============================] - 1502s 27ms/step - loss: -8.0401 - crf_viterbi_accuracy: 0.9899 - val_loss: -1.4728 - val_crf_viterbi_accuracy: 0.9898\n",
      "\n",
      "Epoch 00017: val_crf_viterbi_accuracy did not improve from 0.99092\n",
      "Epoch 18/20\n",
      "55410/55410 [==============================] - 1502s 27ms/step - loss: -9.0157 - crf_viterbi_accuracy: 0.9898 - val_loss: -5.1670 - val_crf_viterbi_accuracy: 0.9902\n",
      "\n",
      "Epoch 00018: val_crf_viterbi_accuracy did not improve from 0.99092\n",
      "Epoch 19/20\n",
      "55410/55410 [==============================] - 1502s 27ms/step - loss: -9.9888 - crf_viterbi_accuracy: 0.9901 - val_loss: -11.9001 - val_crf_viterbi_accuracy: 0.9896\n",
      "\n",
      "Epoch 00019: val_crf_viterbi_accuracy did not improve from 0.99092\n",
      "Epoch 20/20\n",
      "55410/55410 [==============================] - 1502s 27ms/step - loss: -11.2405 - crf_viterbi_accuracy: 0.9903 - val_loss: -4.0062 - val_crf_viterbi_accuracy: 0.9902\n",
      "\n",
      "Epoch 00020: val_crf_viterbi_accuracy did not improve from 0.99092\n"
     ]
    }
   ],
   "source": [
    "filepath=MODEL_PATH+\"weights-improvement-{epoch:02d}-{val_crf_viterbi_accuracy:.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_crf_viterbi_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit([X_word_tr,\n",
    "                     np.array(X_char_tr).reshape((len(X_char_tr), max_len, max_len_char))\n",
    "                     ], \n",
    "                     np.array(y_tr),\n",
    "                     batch_size=train_batch_size, epochs=train_epochs, verbose=1,callbacks=callbacks_list,\n",
    "                     validation_data=(\n",
    "                     [X_word_te,\n",
    "                     np.array(X_char_te).reshape((len(X_char_te), max_len, max_len_char))\n",
    "                     ],\n",
    "                     np.array(y_te))\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aff94c",
   "metadata": {},
   "source": [
    "# Plot Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfefd190",
   "metadata": {},
   "source": [
    "### Accuracy Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57eaf60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = pd.DataFrame(history.history)\n",
    "\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure(figsize=(12,12))\n",
    "# plt.plot(hist[\"crf_viterbi_accuracy\"])\n",
    "# plt.plot(hist[\"val_crf_viterbi_accuracy\"])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601a54f8",
   "metadata": {},
   "source": [
    "###  Loss Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd5d4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist = pd.DataFrame(history.history)\n",
    "\n",
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure(figsize=(12,12))\n",
    "# plt.plot(hist[\"loss\"])\n",
    "# plt.plot(hist[\"val_loss\"])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c8b9a6",
   "metadata": {},
   "source": [
    "# Save Last Weight Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a8ae023",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_filepath=MODEL_PATH+\"last_weight-50.hdf5\"\n",
    "model.save_weights(save_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9baaa1f",
   "metadata": {},
   "source": [
    "# Load Weight Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8cbf7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/crimex/CRIMEX/BiLSTM-CRF/model/Keras/WordCharModel/sent/final/\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d05be1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'str' object has no attribute 'decode'.\n",
    "# navigate to - ~/miniconda3/lib/python3.10/site-packages/keras/engine/topology.py\n",
    "\n",
    "# remove .decode('utf-8') line: 2940, 3339, 3343\n",
    "\n",
    "# ref: https://stackoverflow.com/questions/28583565/str-object-has-no-attribute-decode-python-3-error\n",
    "\n",
    "# load_filepath = MODEL_PATH + path + \"/weights-improvement-18-0.991.hdf5\" \n",
    "load_filepath=MODEL_PATH+'weights-improvement-06-0.991.hdf5'\n",
    "model.load_weights(load_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b595289",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b3f40b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-20 06:35:54.992202: W tensorflow/c/c_api.cc:291] Operation '{name:'crf_1/cond/Identity' id:1165 op device:{requested: '', assigned: ''} def:{{{node crf_1/cond/Identity}} = Identity[T=DT_FLOAT, _has_manual_control_dependencies=true](crf_1/cond)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9336/9336 [==============================] - 59s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_model = model.predict(\n",
    "    [X_word_te,\n",
    "     np.array(X_char_te).reshape(\n",
    "                                 (len(X_char_te),\n",
    "                                  max_len, \n",
    "                                  max_len_char))\n",
    "    ], \n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e79c43df",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(0,len(pred_model)):\n",
    "    try:\n",
    "        out = np.argmax(pred_model[i], axis=-1)\n",
    "        true = np.argmax(y_te[i], axis=-1)\n",
    "        revert_pred=[ix_to_ner[i] for i in out]\n",
    "        revert_true=[ix_to_ner[i] for i in true]\n",
    "        y_pred.append(revert_pred)\n",
    "        y_test.append(revert_true)\n",
    "    except:\n",
    "        print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015fd551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "817069f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import numpy\n",
    "from itertools import chain\n",
    "\n",
    "flatten_true_predictions = []\n",
    "flatten_true_labels = []\n",
    "\n",
    "for i in range (0,len(y_test)):\n",
    "#     print(y_test[i])\n",
    "    for k in range(0,len(y_test[i])):\n",
    "#         print(y_test[i][k])\n",
    "        if y_test[i][k] != 'pad'and y_pred[i][k] != 'pad': \n",
    "            flatten_true_predictions.append(y_pred[i][k])\n",
    "            flatten_true_labels.append(y_test[i][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57973435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------+\n",
      "| MCC report \n",
      "+--------------+-----------------------+\n",
      "| Action       |  0.07113231946796235\n",
      "| Criminal     |  0.6303508260494857\n",
      "| Datetime     |  0.3389507125930282\n",
      "| Enforcement  |  0.5380707561567631\n",
      "| Item         |  0.6405760232862372\n",
      "| Location     |  0.7037548309320654\n",
      "| O            |  0.544557129002601\n",
      "| Rootcause    |  0.08885301785857623\n",
      "| Trigger      |  0.038553571697930174\n",
      "| Victim       |  0.5573909124204773\n",
      "| Worth        |  0.4364316529377991\n",
      "+---------------+-----------------------+\n",
      "| Overall      |  0.5284769960356585\n",
      "+---------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
    "tag_map = {'B-Criminal': 'Criminal',\n",
    "           'I-Criminal': 'Criminal',\n",
    "           'B-Action': 'Action',\n",
    "           'I-Action': 'Action',\n",
    "           'B-Location': 'Location',\n",
    "           'I-Location': 'Location',\n",
    "           'B-Item': 'Item',\n",
    "           'I-Item': 'Item',\n",
    "           'B-Victim': 'Victim',\n",
    "           'I-Victim': 'Victim',\n",
    "           'B-worth': 'Worth',\n",
    "           'I-worth': 'Worth',\n",
    "           'B-Datetime': 'Datetime',\n",
    "           'I-Datetime': 'Datetime',\n",
    "           'B-Enforcement': 'Enforcement',\n",
    "           'I-Enforcement': 'Enforcement',\n",
    "           'B-Rootcause': 'Rootcause',\n",
    "           'I-Rootcause': 'Rootcause',\n",
    "           'B-Trigger': 'Trigger',\n",
    "           'I-Trigger': 'Trigger',\n",
    "           'O': 'O'}\n",
    "\n",
    "# Convert the original tags to merged tags\n",
    "merged_true_labels = [tag_map[tag] for tag in flatten_true_labels]\n",
    "merged_true_predictions = [tag_map[tag] for tag in flatten_true_predictions]\n",
    "\n",
    "ny_true = np.array(merged_true_labels)\n",
    "ny_pred = np.array(merged_true_predictions)\n",
    "\n",
    "# mcc report (str)\n",
    "mcc_report = \"+--------------+-----------------------+\\n\"\n",
    "mcc_report += f\"| MCC report \\n\"\n",
    "mcc_report += \"+--------------+-----------------------+\\n\"\n",
    "\n",
    "# Calculate MCC for all labels\n",
    "mcc_all = matthews_corrcoef(ny_true, ny_pred)\n",
    "# print(\"MCC (all labels):\", mcc_all)\n",
    "\n",
    "# Calculate MCC for each label\n",
    "for label in np.unique(ny_true):\n",
    "    y_true_label = np.where(ny_true == label, 1, 0)\n",
    "    y_pred_label = np.where(ny_pred == label, 1, 0)\n",
    "    mcc_label = matthews_corrcoef(y_true_label, y_pred_label)\n",
    "#     print(f\"MCC (label {label}):\", mcc_label)\n",
    "    mcc_report += f\"| {label:<12} |  {mcc_label}\\n\"\n",
    "    \n",
    "mcc_report += \"+---------------+-----------------------+\\n\"\n",
    "mcc_report += f\"| {'Overall':<12} |  {mcc_all}\\n\"\n",
    "mcc_report += \"+---------------+-----------------------+\\n\"\n",
    "print(mcc_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f2f9c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------+\n",
      "| ACC report \n",
      "+--------------+-----------------------+\n",
      "| Action       |  0.9868824384781805\n",
      "| Criminal     |  0.9856917753041258\n",
      "| Datetime     |  0.9912194924228223\n",
      "| Enforcement  |  0.9829304500706798\n",
      "| Item         |  0.9857779083848021\n",
      "| Location     |  0.9912042924674087\n",
      "| O            |  0.9023301531648841\n",
      "| Rootcause    |  0.9936261520299541\n",
      "| Trigger      |  0.9924456221595084\n",
      "| Victim       |  0.9887824329048635\n",
      "| Worth        |  0.9859248412871322\n",
      "+---------------+-----------------------+\n",
      "| Overall      |  0.9806195962431236\n",
      "+---------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_all = dict()\n",
    "for label in np.unique(ny_true):\n",
    "    y_true_label = np.where(ny_true == label, 1, 0)\n",
    "    y_pred_label = np.where(ny_pred == label, 1, 0)\n",
    "    accuracy = accuracy_score(y_true_label, y_pred_label)\n",
    "    accuracy_all[label] = accuracy\n",
    "\n",
    "# print(\"Accuracy (all labels):\", np.mean(list(accuracy_all.values())))\n",
    "\n",
    "# acc report (str)\n",
    "acc_report = \"+--------------+-----------------------+\\n\"\n",
    "acc_report += f\"| ACC report \\n\"\n",
    "acc_report += \"+--------------+-----------------------+\\n\"\n",
    "\n",
    "for label, accuracy in accuracy_all.items():\n",
    "#     print(f\"Accuracy (label {label}):\", accuracy)\n",
    "    acc_report += f\"| {label:<12} |  {accuracy}\\n\"\n",
    "    \n",
    "acc_report += \"+---------------+-----------------------+\\n\"\n",
    "acc_report += f\"| {'Overall':<12} |  {np.mean(list(accuracy_all.values()))}\\n\"\n",
    "acc_report += \"+---------------+-----------------------+\\n\"\n",
    "print(acc_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cfcea3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Action       0.18      0.03      0.05      2335\n",
      "    Criminal       0.66      0.62      0.64      4002\n",
      "    Datetime       0.52      0.23      0.32      1758\n",
      " Enforcement       0.49      0.61      0.54      3277\n",
      "        Item       0.66      0.64      0.65      4043\n",
      "    Location       0.82      0.61      0.70      3321\n",
      "           O       0.92      0.97      0.94    170062\n",
      "   Rootcause       0.33      0.02      0.05      1228\n",
      "     Trigger       0.45      0.00      0.01      1490\n",
      "      Victim       0.61      0.52      0.56      2723\n",
      "       Worth       0.61      0.32      0.42      3130\n",
      "\n",
      "    accuracy                           0.89    197369\n",
      "   macro avg       0.57      0.42      0.44    197369\n",
      "weighted avg       0.88      0.89      0.88    197369\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(ny_true, ny_pred, labels=np.unique(ny_true))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7780a7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/crimex/CRIMEX/BiLSTM-CRF/report_sent_final.txt\n"
     ]
    }
   ],
   "source": [
    "print(save_txt)\n",
    "with open(save_txt, 'w') as file:\n",
    "    \n",
    "    defi = 'Train by ' + str(len(y_test)) + ' sents &&& Test by ' + str(len(y_pred)) + ' sents\\n'\n",
    "    file.write(defi)\n",
    "\n",
    "#     file.write('classification_report\\n\\n')\n",
    "#     file.write(classi_report)\n",
    "    file.write('\\n-----------------------------------------------------------------\\n\\n')\n",
    "    file.write('classification_report\\n')\n",
    "    file.write(report)\n",
    "    file.write('\\n-----------------------------------------------------------------\\n\\n\\n')\n",
    "    file.write(mcc_report)\n",
    "    file.write('\\n-----------------------------------------------------------------\\n\\n\\n')\n",
    "    file.write(acc_report)\n",
    "    \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a16828d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: XlsxWriter in /home/crimex/miniconda3/lib/python3.10/site-packages (3.0.9)\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92880197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/crimex/CRIMEX/predicted_bert/final_sent_bilstm.xlsx\n"
     ]
    }
   ],
   "source": [
    "import xlsxwriter\n",
    "print(save_result_xls)\n",
    "workbook = xlsxwriter.Workbook(save_result_xls)\n",
    "worksheet = workbook.add_worksheet(\"My sheet\")\n",
    "row = 0\n",
    "col = 0\n",
    "for i in y_pred:\n",
    "    str1 = str(i)\n",
    "    worksheet.write(row, 0 , str1)\n",
    "    row += 1\n",
    "row = 0\n",
    "for i in y_test:\n",
    "    str2 = str(i)\n",
    "    worksheet.write(row, 1 , str2)\n",
    "    row += 1\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47bb43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f20579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
