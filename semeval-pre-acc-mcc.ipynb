{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b892b003",
   "metadata": {},
   "source": [
    "ref: https://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5f2256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# run libery location py file\n",
    "%run /home/crimex/CRIMEX/ner_eval.py\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d60180a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/crimex/CRIMEX/predicted_bert/xlmr_std_sent.xlsx\n",
      "/home/crimex/CRIMEX/predicted_bert/semeval_result/xlmr_std_sent_semeval.txt \n",
      "-----------\n",
      "/home/crimex/CRIMEX/predicted_bert/final_sent_bilstm.xlsx\n",
      "/home/crimex/CRIMEX/predicted_bert/semeval_result/final_sent_bilstm_semeval.txt \n",
      "-----------\n",
      "/home/crimex/CRIMEX/predicted_bert/wang_last.xlsx\n",
      "/home/crimex/CRIMEX/predicted_bert/semeval_result/wang_last_semeval.txt \n",
      "-----------\n",
      "/home/crimex/CRIMEX/predicted_bert/xlmr_last.xlsx\n",
      "/home/crimex/CRIMEX/predicted_bert/semeval_result/xlmr_last_semeval.txt \n",
      "-----------\n",
      "/home/crimex/CRIMEX/predicted_bert/final_sent_crf.xlsx\n",
      "/home/crimex/CRIMEX/predicted_bert/semeval_result/final_sent_crf_semeval.txt \n",
      "-----------\n",
      "/home/crimex/CRIMEX/predicted_bert/std_chunk.xlsx\n",
      "/home/crimex/CRIMEX/predicted_bert/semeval_result/std_chunk_semeval.txt \n",
      "-----------\n",
      "-------------FINISH------------\n"
     ]
    }
   ],
   "source": [
    "from ner_eval import Evaluator\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dir_path = '/home/crimex/CRIMEX/predicted_result/'\n",
    "\n",
    "ent_type_eval = ['Criminal', 'Victim', 'Action', 'Location', 'Datetime',  'Item', \n",
    "                 'Rootcause', 'Trigger','worth', 'Enforcement']\n",
    "\n",
    "for file_name in os.listdir(dir_path):\n",
    "    if file_name.endswith('.xlsx'):\n",
    "        file_path = os.path.join(dir_path, file_name)\n",
    "        print(file_path)\n",
    "        \n",
    "        # read xlsx file \n",
    "        df = pd.read_excel(file_path)\n",
    "        df.columns = ['Predict', 'Real']\n",
    "        \n",
    "        # get test dataset \n",
    "        y_pred = []\n",
    "        y_test = []\n",
    "\n",
    "        for ind in range(0, df.shape[0]):\n",
    "            y_pred.append(ast.literal_eval(df['Predict'][ind]))\n",
    "            y_test.append(ast.literal_eval(df['Real'][ind]))\n",
    "            \n",
    "        # infor eval ------------------------------------------------------------------------------\n",
    "        evaluator = Evaluator(y_test, y_pred, ent_type_eval)\n",
    "        results, results_agg = evaluator.evaluate()\n",
    "        \n",
    "        \n",
    "        # token eval ------------------------------------------------------------------------------\n",
    "#         flatten_true_predictions= list(chain.from_iterable(y_pred))\n",
    "#         flatten_true_labels= list(chain.from_iterable(y_test))\n",
    "        flatten_true_predictions = []\n",
    "        flatten_true_labels = []\n",
    "\n",
    "        for i in range (0,len(y_test)):\n",
    "        #     print(y_test[i])\n",
    "            for k in range(0,len(y_test[i])):\n",
    "        #         print(y_test[i][k])\n",
    "                if y_test[i][k] != 'pad'and y_pred[i][k] != 'pad': \n",
    "                    flatten_true_predictions.append(y_pred[i][k])\n",
    "                    flatten_true_labels.append(y_test[i][k])\n",
    "        # printing result\n",
    "        r = sklearn.metrics.confusion_matrix(flatten_true_labels, flatten_true_predictions)\n",
    "        r\n",
    "        cm = r.astype('float') / r.sum(axis=1)[:, np.newaxis]\n",
    "        # cm\n",
    "        # cm.diagonal()\n",
    "        \n",
    "        # MCC -------------------\n",
    "        from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
    "        tag_map = {'B-Criminal': 'Criminal',\n",
    "                   'I-Criminal': 'Criminal',\n",
    "                   'B-Action': 'Action',\n",
    "                   'I-Action': 'Action',\n",
    "                   'B-Location': 'Location',\n",
    "                   'I-Location': 'Location',\n",
    "                   'B-Item': 'Item',\n",
    "                   'I-Item': 'Item',\n",
    "                   'B-Victim': 'Victim',\n",
    "                   'I-Victim': 'Victim',\n",
    "                   'B-worth': 'Worth',\n",
    "                   'I-worth': 'Worth',\n",
    "                   'B-Datetime': 'Datetime',\n",
    "                   'I-Datetime': 'Datetime',\n",
    "                   'B-Enforcement': 'Enforcement',\n",
    "                   'I-Enforcement': 'Enforcement',\n",
    "                   'B-Rootcause': 'Rootcause',\n",
    "                   'I-Rootcause': 'Rootcause',\n",
    "                   'B-Trigger': 'Trigger',\n",
    "                   'I-Trigger': 'Trigger',\n",
    "                   'O': 'O'}\n",
    "\n",
    "        # Convert the original tags to merged tags\n",
    "        merged_true_labels = [tag_map[tag] for tag in flatten_true_labels]\n",
    "        merged_true_predictions = [tag_map[tag] for tag in flatten_true_predictions]\n",
    "\n",
    "        ny_true = np.array(merged_true_labels)\n",
    "        ny_pred = np.array(merged_true_predictions)\n",
    "\n",
    "        # mcc report (str)\n",
    "        mcc_report = \"+--------------+-----------------------+\\n\"\n",
    "        mcc_report += f\"| MCC report \\n\"\n",
    "        mcc_report += \"+--------------+-----------------------+\\n\"\n",
    "\n",
    "        # Calculate MCC for all labels\n",
    "        mcc_all = matthews_corrcoef(ny_true, ny_pred)\n",
    "        # print(\"MCC (all labels):\", mcc_all)\n",
    "        \n",
    "\n",
    "        # Calculate MCC for each label\n",
    "        for label in np.unique(ny_true):\n",
    "            y_true_label = np.where(ny_true == label, 1, 0)\n",
    "            y_pred_label = np.where(ny_pred == label, 1, 0)\n",
    "            mcc_label = matthews_corrcoef(y_true_label, y_pred_label)\n",
    "        #     print(f\"MCC (label {label}):\", mcc_label)\n",
    "            mcc_report += f\"| {label:<12} |  {mcc_label}\\n\"\n",
    "\n",
    "        mcc_report += \"+---------------+-----------------------+\\n\"\n",
    "        mcc_report += f\"| {'Overall':<12} |  {mcc_all}\\n\"\n",
    "        mcc_report += \"+---------------+-----------------------+\\n\"\n",
    "#         print(mcc_report)\n",
    "        \n",
    "        # ACC -------------------\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        accuracy_all = dict()\n",
    "        for label in np.unique(ny_true):\n",
    "            y_true_label = np.where(ny_true == label, 1, 0)\n",
    "            y_pred_label = np.where(ny_pred == label, 1, 0)\n",
    "            accuracy = accuracy_score(y_true_label, y_pred_label)\n",
    "            accuracy_all[label] = accuracy\n",
    "        # print(\"Accuracy (all labels):\", np.mean(list(accuracy_all.values())))\n",
    "\n",
    "        acc_report = \"+--------------+-----------------------+\\n\"\n",
    "        acc_report += f\"| ACC report \\n\"\n",
    "        acc_report += \"+--------------+-----------------------+\\n\"\n",
    "\n",
    "        for label, accuracy in accuracy_all.items():\n",
    "        #     print(f\"Accuracy (label {label}):\", accuracy)\n",
    "            acc_report += f\"| {label:<12} |  {accuracy}\\n\"\n",
    "\n",
    "        acc_report += \"+---------------+-----------------------+\\n\"\n",
    "        acc_report += f\"| {'Overall':<12} |  {np.mean(list(accuracy_all.values()))}\\n\"\n",
    "        acc_report += \"+---------------+-----------------------+\\n\"\n",
    "#         print(acc_report)\n",
    "\n",
    "        # precision -------------------\n",
    "        from sklearn.metrics import precision_score\n",
    "        precision_all = dict()\n",
    "        for label in np.unique(ny_true):\n",
    "            y_true_label = np.where(ny_true == label, 1, 0)\n",
    "            y_pred_label = np.where(ny_pred == label, 1, 0)\n",
    "            precision = precision_score(y_true_label, y_pred_label)\n",
    "            precision_all[label] = precision\n",
    "        # print(\"Accuracy (all labels):\", np.mean(list(accuracy_all.values())))\n",
    "\n",
    "        pre_report = \"+--------------+-----------------------+\\n\"\n",
    "        pre_report += f\"| precison report \\n\"\n",
    "        pre_report += \"+--------------+-----------------------+\\n\"\n",
    "\n",
    "        for label, precision in precision_all.items():\n",
    "        #     print(f\"Accuracy (label {label}):\", accuracy)\n",
    "            pre_report += f\"| {label:<12} |  {precision}\\n\"\n",
    "\n",
    "        pre_report += \"+---------------+-----------------------+\\n\"\n",
    "        pre_report += f\"| {'Overall':<12} |  {np.mean(list(accuracy_all.values()))}\\n\"\n",
    "        pre_report += \"+---------------+-----------------------+\\n\"\n",
    "#         print(pre_report)\n",
    "\n",
    "        from sklearn.metrics import classification_report\n",
    "        report = classification_report(ny_true, ny_pred, labels=np.unique(ny_true))\n",
    "#         print(report)\n",
    "\n",
    "        # save result eval ------------------------------------------------------------------------------\n",
    "        re_path = dir_path+'semeval_result/' + file_name[:-5]+'_semeval.txt'\n",
    "        print(re_path, '\\n-----------')\n",
    "        \n",
    "        with open(re_path , 'w') as file:   \n",
    "            file.write('\\n-----------------------------------------------------------------\\n\\n')\n",
    "            file.write('classification_report\\n')\n",
    "            file.write(report)\n",
    "            file.write('\\n-----------------------------------------------------------------\\n\\n')\n",
    "            file.write(mcc_report)\n",
    "            file.write('\\n-----------------------------------------------------------------\\n\\n')\n",
    "            file.write(acc_report)\n",
    "            file.write('\\n-----------------------------------------------------------------\\n\\n')\n",
    "            file.write(pre_report)\n",
    "            file.write('\\n-----------------------------------------------------------------\\n\\nOver All\\n')\n",
    "            for re in results:\n",
    "                file.write(re)\n",
    "                file.write('\\n')\n",
    "                file.write(str(results[re]))\n",
    "                file.write('\\n')\n",
    "\n",
    "            file.write('\\n-----------------------------------------------------------------\\n\\nEach Labels\\n\\n')\n",
    "            for re_agg in results_agg:\n",
    "                file.write(re_agg)\n",
    "                file.write('\\n')\n",
    "                for re in results_agg[re_agg]: \n",
    "                    file.write(re)\n",
    "                    file.write('\\n')\n",
    "                    file.write(str(results_agg[re_agg][re]))\n",
    "                    file.write('\\n')\n",
    "                file.write('\\n\\n')\n",
    "            file.close()\n",
    "        \n",
    "print(\"-------------FINISH------------\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49e5750d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------FINISH------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------FINISH------------\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e33c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a71335e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be5a436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
