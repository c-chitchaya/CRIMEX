{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a12ca904",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# run libery location py file\n",
    "%run /home/crimex/CRIMEX/ner_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d1f1fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db135181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_eval import Evaluator\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcf660d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c603bb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/crimex/CRIMEX/predicted_bert/500_xlmr.xlsx\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/crimex/CRIMEX/predicted_bert/500_xlmr.xlsx'\n",
    "print(file_path)\n",
    "\n",
    "ent_type_eval = ['Criminal', 'Victim', 'Action', 'Location', 'Datetime',  'Item', \n",
    "                 'Rootcause', 'Trigger','worth', 'Enforcement']\n",
    "\n",
    "# read xlsx file \n",
    "df = pd.read_excel(file_path)\n",
    "df.columns = ['Predict', 'Real']\n",
    "\n",
    "# get test dataset \n",
    "y_pred = []\n",
    "y_test = []\n",
    "\n",
    "for ind in range(0, df.shape[0]):\n",
    "    y_pred.append(ast.literal_eval(df['Predict'][ind]))\n",
    "    y_test.append(ast.literal_eval(df['Real'][ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55ddc890",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(y_test, y_pred, ent_type_eval)\n",
    "results, results_agg = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbca960b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ent_type': {'correct': 845,\n",
       "  'incorrect': 112,\n",
       "  'partial': 0,\n",
       "  'missed': 1444,\n",
       "  'spurious': 2004,\n",
       "  'possible': 2401,\n",
       "  'actual': 2961,\n",
       "  'precision': 0.28537656197230665,\n",
       "  'recall': 0.35193669304456476,\n",
       "  'f1-score': 0.3151809026482656,\n",
       "  'accuracy': 0.19182746878547105,\n",
       "  'tp': 0,\n",
       "  'tn': 0,\n",
       "  'fp': 0,\n",
       "  'fn': 0,\n",
       "  'mcc': 0,\n",
       "  'aug-roc': 0},\n",
       " 'partial': {'correct': 483,\n",
       "  'incorrect': 0,\n",
       "  'partial': 474,\n",
       "  'missed': 1444,\n",
       "  'spurious': 2004,\n",
       "  'possible': 2401,\n",
       "  'actual': 2961,\n",
       "  'precision': 0.24316109422492402,\n",
       "  'recall': 0.299875052061641,\n",
       "  'f1-score': 0.268556508765386,\n",
       "  'accuracy': 0.10964812712826334,\n",
       "  'tp': 0,\n",
       "  'tn': 0,\n",
       "  'fp': 0,\n",
       "  'fn': 0,\n",
       "  'mcc': 0,\n",
       "  'aug-roc': 0},\n",
       " 'strict': {'correct': 436,\n",
       "  'incorrect': 521,\n",
       "  'partial': 0,\n",
       "  'missed': 1444,\n",
       "  'spurious': 2004,\n",
       "  'possible': 2401,\n",
       "  'actual': 2961,\n",
       "  'precision': 0.14724755150287064,\n",
       "  'recall': 0.18159100374843815,\n",
       "  'f1-score': 0.16262588586348375,\n",
       "  'accuracy': 0.09897843359818388,\n",
       "  'tp': 0,\n",
       "  'tn': 0,\n",
       "  'fp': 0,\n",
       "  'fn': 0,\n",
       "  'mcc': 0,\n",
       "  'aug-roc': 0},\n",
       " 'exact': {'correct': 483,\n",
       "  'incorrect': 474,\n",
       "  'partial': 0,\n",
       "  'missed': 1444,\n",
       "  'spurious': 2004,\n",
       "  'possible': 2401,\n",
       "  'actual': 2961,\n",
       "  'precision': 0.16312056737588654,\n",
       "  'recall': 0.20116618075801748,\n",
       "  'f1-score': 0.18015665796344646,\n",
       "  'accuracy': 0.10964812712826334,\n",
       "  'tp': 0,\n",
       "  'tn': 0,\n",
       "  'fp': 0,\n",
       "  'fn': 0,\n",
       "  'mcc': 0,\n",
       "  'aug-roc': 0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd439e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4786abb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5683b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340a9909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "dir_path = '/home/crimex/CRIMEX/predicted_bert/'\n",
    "\n",
    "ent_type_eval = ['Criminal', 'Victim', 'Action', 'Location', 'Datetime',  'Item', \n",
    "                 'Rootcause', 'Trigger','worth', 'Enforcement']\n",
    "\n",
    "for file_name in os.listdir(dir_path):\n",
    "    if file_name.endswith('.xlsx'):\n",
    "        file_path = os.path.join(dir_path, file_name)\n",
    "        print(file_path)\n",
    "        \n",
    "        # read xlsx file \n",
    "        df = pd.read_excel(file_path)\n",
    "        df.columns = ['Predict', 'Real']\n",
    "        \n",
    "        # get test dataset \n",
    "        y_pred = []\n",
    "        y_test = []\n",
    "\n",
    "        for ind in range(0, df.shape[0]):\n",
    "            y_pred.append(ast.literal_eval(df['Predict'][ind]))\n",
    "            y_test.append(ast.literal_eval(df['Real'][ind]))\n",
    "        \n",
    "        evaluator = Evaluator(y_test, y_pred, ent_type_eval)\n",
    "        results, results_agg = evaluator.evaluate()\n",
    "        \n",
    "        re_path = dir_path+'/semeval_result/' + file_name[:-5]+'_semeval.txt'\n",
    "        print(re_path, '\\n-----------')\n",
    "#         df2_result = pd.DataFrame.from_dict(results)\n",
    "#         df2_result.T\n",
    "#         df2_result.to_excel(re_path, index_label='Metric')\n",
    "\n",
    "        with open(re_path , 'w') as file:    \n",
    "            file.write('\\n-----------------------------------------------------------------\\n\\nOver All\\n\\n')\n",
    "            for re in results:\n",
    "                file.write(re)\n",
    "                file.write('\\n')\n",
    "                file.write(str(results[re]))\n",
    "                file.write('\\n')\n",
    "\n",
    "            file.write('\\n-----------------------------------------------------------------\\n\\nEach Labels\\n\\n')\n",
    "            for re_agg in results_agg:\n",
    "                file.write(re_agg)\n",
    "                file.write('\\n')\n",
    "                for re in results_agg[re_agg]: \n",
    "                    file.write(re)\n",
    "                    file.write('\\n')\n",
    "                    file.write(str(results_agg[re_agg][re]))\n",
    "                    file.write('\\n')\n",
    "                file.write('\\n\\n')\n",
    "            file.close()\n",
    "        \n",
    "        \n",
    "print(\"-------------FINISH------------\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd11f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9819db26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a973e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2285a624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f1d42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba0e6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b132fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '/home/crimex/CRIMEX/predicted_bert/1000_chunk_wang.xlsx'\n",
    "print(a[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b9319",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7eabc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas dataframe with some data\n",
    "data = {'Name': ['Alice', 'Bob', 'Charlie', 'Dave'],\n",
    "        'Age': [25, 30, 35, 40],\n",
    "        'Salary': [50000, 60000, 70000, 80000]}\n",
    "df2 = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc87b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dataframe to an xlsx file\n",
    "df4 = pd.DataFrame.from_dict(results)\n",
    "df4.T\n",
    "df4.to_excel(dir_path+'/output.xlsx', index_label='Metric')\n",
    "# df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf5a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = pd.read_excel(dir_path+'/output.xlsx')\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame from the dictionary\n",
    "df7 = pd.DataFrame.from_dict({(i,j): results_agg[i][j] \n",
    "                             for i in results_agg.keys() \n",
    "                             for j in results_agg[i].keys()},\n",
    "                            orient='index')\n",
    "\n",
    "# # create a multi-level index for the DataFrame\n",
    "df7.index = pd.MultiIndex.from_tuples(df7.index)\n",
    "\n",
    "# # create a new DataFrame with all the information in separate columns\n",
    "df_new = pd.DataFrame(df7[0].tolist(), index=df7.index).reset_index()\n",
    "\n",
    "# # rename the columns of the new DataFrame\n",
    "# df_new.columns = ['Entity', 'Evaluation Type', 'Correct', 'Incorrect', 'Partial', 'Missed', 'Spurious', 'Possible', 'Actual', 'Precision', 'Recall']\n",
    "\n",
    "# # write the DataFrame to an xlsx file\n",
    "df7.to_excel(dir_path+'/evaluation.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df7.columns = ['Entity', 'type', 'correct', 'incorrect', 'partial', 'missed', 'spurious', 'possible', 'actual', 'precision', 'recall']\n",
    "df7[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ae050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df7\n",
    "df8 = pd.read_excel(dir_path+'/evaluation.xlsx')\n",
    "df8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1865bcc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674b9fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
