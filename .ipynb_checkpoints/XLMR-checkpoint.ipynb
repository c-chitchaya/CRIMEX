{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd90933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 14:21:42.400220: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 14:21:42.908946: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-07 14:21:42.908990: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-04-07 14:21:42.908994: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "#datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "#transformers\n",
    "from transformers import (\n",
    "    CamembertTokenizer,\n",
    "    \n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fe320c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd223e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a1252da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/home/crimex/CRIMEX/Dataset/XLMR_train2.csv')\n",
    "df.head()\n",
    "df = df[df['tag'].notnull()]\n",
    "type(df['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47a60809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-Criminal', 'I-Item', 'I-Enforcement', 'B-Location', 'I-Location', 'B-Victim', 'B-Item', 'B-Datetime', 'I-worth', 'I-Rootcause', 'O', 'B-worth', 'I-Action', 'B-Enforcement', 'I-Trigger', 'I-Criminal', 'B-Trigger', 'B-Action', 'B-Rootcause', 'I-Datetime', 'I-Victim'}\n",
      "{'B-Action': 0, 'B-Criminal': 1, 'B-Datetime': 2, 'B-Enforcement': 3, 'B-Item': 4, 'B-Location': 5, 'B-Rootcause': 6, 'B-Trigger': 7, 'B-Victim': 8, 'B-worth': 9, 'I-Action': 10, 'I-Criminal': 11, 'I-Datetime': 12, 'I-Enforcement': 13, 'I-Item': 14, 'I-Location': 15, 'I-Rootcause': 16, 'I-Trigger': 17, 'I-Victim': 18, 'I-worth': 19, 'O': 20}\n"
     ]
    }
   ],
   "source": [
    "labels = [i.split() for i in df['tag'].values.tolist()]\n",
    "\n",
    "# Check how many labels are there in the dataset\n",
    "unique_labels = set()\n",
    "\n",
    "for lb in labels:\n",
    "  [unique_labels.add(i) for i in lb if i not in unique_labels]\n",
    " \n",
    "print(unique_labels)\n",
    "\n",
    "# Map each label into its id representation and vice versa\n",
    "labels_to_ids = {k: v for v, k in enumerate(sorted(unique_labels))}\n",
    "ids_to_labels = {v: k for v, k in enumerate(sorted(unique_labels))}\n",
    "print(labels_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dbe0f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XLMR\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertForTokenClassification, XLMRobertaTokenizerFast\n",
    "from transformers import CamembertTokenizerFast\n",
    "example = 'โรงเรียนสวนกุหลาบเป็นโรงเรียนที่ดี แต่ไม่มีสวนกุหลาบ'\n",
    "\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\n",
    "                                  \"xlm-roberta-base\",\n",
    "                                  revision='main')\n",
    "#padding\n",
    "text_tokenized = tokenizer(df['text'][0],  max_length=512, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7292cc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁[', ':', ':', 'DES', 'C', ':', ':', ']', 'เมื่อ', '▁วันที่', '▁8', '▁ก', '.', 'ค', '▁', '.', '▁พ', '.', 'อ', '.', 'บุญ', 'สิน', '▁', 'พา', 'ด', 'กลาง', '▁ผ', 'บ', '.', 'กอง', 'กําลัง', 'รักษา', 'ความ', 'สงบ', 'เรียบร้อย', '▁จ', '.', 'มุก', 'ดา', 'หาร', '▁พ', '.', 'ต', '.', 'สิทธิ์', 'ศักดิ์', '▁', 'สิทธิ์', 'โคตร', '▁', 'หัวหน้า', 'ชุด', 'ป', 'ฎ', 'ิ', 'บั', 'ติ', 'การ', 'พิเศษ', 'กอง', 'กําลัง', 'รักษา', 'ความ', 'สงบ', 'เรียบร้อย', '▁จ', '.', 'มุก', 'ดา', 'หาร', '▁', 'ร่วมกับ', 'ต', 'ช', 'ด', '.', 'ที่', '▁234', '▁', 'มุก', 'ดา', 'หาร', '▁', 'จู', '่', 'โจ', 'ม', 'ปิด', 'ล้อม', 'การ', '▁', 'ลัก', 'ล', 'อบ', 'เล่น', 'ไพ่', '▁', 'บริเวณ', '▁บ้าน', 'เลข', 'ที่', '▁12', '▁', 'หมู่', '▁3', '▁ต', '.', 'บาง', 'ทราย', 'ใหญ่', '▁รวมทั้ง', 'โพ', 'ย', 'อ้าง', 'การ', 'จ่าย', 'ส', '่ว', 'ย', 'ให้กับ', 'เจ้าหน้าที่', 'รัฐ', '▁โดยมี', '▁นาง', 'วา', 'ส', 'นา', '▁', 'ทอง', 'มหา', '▁อายุ', '▁48', '▁ปี', '▁รับ', 'ว่า', 'เป็นเจ้าของ', 'บ้าน', '▁นอกจากนี้', 'ทหาร', 'ชุด', 'ดังกล่าว', 'ยัง', 'บุก', 'จับ', 'กุม', 'การ', 'ลัก', 'ล', 'อบ', 'เล่น', 'การ', 'พนัน', 'ในพื้นที่', 'บ้าน', 'ตา', 'ล', 'ใหม่', '▁อ', '.', 'ดอน', 'ตา', 'ล', '▁จ', '.', 'มุก', 'ดา', 'หาร', '▁ได้', 'นัก', 'พนัน', '▁16', '▁คน', '▁พร้อม', 'อุปกรณ์', 'เล่น', 'ไฮ', 'โล', 'ครบ', 'ชุด', '▁', 'รี', 'โม', 'ท', 'บังคับ', 'ลูก', 'ไฮ', 'โล', '▁1', '▁ชุด', '▁', 'เงินสด', '▁จึง', 'นํา', 'ตัว', 'ส่ง', 'ให้', 'พนักงาน', 'สอบสวน', '▁', 'ดําเนิน', 'คดี', 'ไป', 'ตาม', 'กฎหมาย', '.', '</s>']\n",
      "207\n",
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 3, 3, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 10, 10, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 14, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 18, 18, 19, 19, 19, 20, 21, 21, 22, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 25, 26, 26, 26, 26, 27, 27, 27, 28, 29, 30, 31, 31, 31, 31, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 33, 33, 33, 34, 34, 34, 34, 34, 35, 35, 35, 36, 37, 38, 38, 38, 38, 38, 38, 38, 39, 39, 39, 39, 39, 39, 39, 39, 40, 41, 42, 42, 43, 43, 43, 43, 43, 43, 43, 44, 44, 44, 44, 44, 44, 44, None]\n"
     ]
    }
   ],
   "source": [
    "word_ids = text_tokenized.word_ids()\n",
    "print(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0]))\n",
    "print(len(tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0])))\n",
    "print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c24dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_label(texts, labels):\n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]])\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(labels_to_ids[labels[word_idx]] if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids\n",
    "\n",
    "class DataSequence(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        lb = [i.split() for i in df['tag'].values.tolist()]\n",
    "        txt = df['text'].values.tolist()\n",
    "        self.texts = [tokenizer(str(i),\n",
    "                               padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\") for i in txt]\n",
    "        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_data(self, idx):\n",
    "\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "\n",
    "        return torch.LongTensor(self.labels[idx])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_data = self.get_batch_data(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "884f07f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195\n",
      "1561\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df = df[0:2000]\n",
    "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42),\n",
    "                            [int(.8 * len(df)), int(.9* len(df))])\n",
    "df_train=df_train.dropna(how='all')\n",
    "df_train=df_train.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "df_test=df_test.dropna(how='all') \n",
    "df_test=df_test.dropna().reset_index(drop=True)\n",
    "\n",
    "df=df.dropna(how='all') \n",
    "df=df.dropna().reset_index(drop=True)\n",
    "print(len(df_val))\n",
    "print(len(df_train))\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa500076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test for XLMR\n",
    "from transformers import BertForTokenClassification, XLMRobertaForTokenClassification\n",
    "\n",
    "class BertModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(BertModel, self).__init__()\n",
    "\n",
    "        self.bert = XLMRobertaForTokenClassification.from_pretrained( \"xlm-roberta-base\",num_labels=len(unique_labels),ignore_mismatched_sizes=True)\n",
    "\n",
    "    def forward(self, input_id, mask, label):\n",
    "\n",
    "        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34b94251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_optimizer in /home/crimex/miniconda3/lib/python3.10/site-packages (0.3.0)\r\n",
      "Requirement already satisfied: torch>=1.5.0 in /home/crimex/miniconda3/lib/python3.10/site-packages (from torch_optimizer) (1.13.1)\r\n",
      "Requirement already satisfied: pytorch-ranger>=0.1.1 in /home/crimex/miniconda3/lib/python3.10/site-packages (from torch_optimizer) (0.1.1)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/crimex/miniconda3/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (11.10.3.66)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/crimex/miniconda3/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (11.7.99)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/crimex/miniconda3/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (8.5.0.96)\r\n",
      "Requirement already satisfied: typing-extensions in /home/crimex/miniconda3/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (4.5.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/crimex/miniconda3/lib/python3.10/site-packages (from torch>=1.5.0->torch_optimizer) (11.7.99)\r\n",
      "Requirement already satisfied: setuptools in /home/crimex/miniconda3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.5.0->torch_optimizer) (59.5.0)\r\n",
      "Requirement already satisfied: wheel in /home/crimex/miniconda3/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.5.0->torch_optimizer) (0.37.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d67bf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForTokenClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f4ab0603664d9d8df5e1fe5abc7312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Loss:  1.126 | Accuracy:  0.802 | Val_Loss:  1.127 | Accuracy:  0.782\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec59fe564224981829c4d9a1d6c4c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Loss:  1.084 | Accuracy:  0.805 | Val_Loss:  1.210 | Accuracy:  0.803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af962c6518e141fc8b4b4411d9532a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Loss:  1.112 | Accuracy:  0.805 | Val_Loss:  1.153 | Accuracy:  0.803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5445610a936496e872bb0617dd71d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Loss:  1.106 | Accuracy:  0.805 | Val_Loss:  1.122 | Accuracy:  0.803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65367c41a3b94622a3f5abd4fd827682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Loss:  1.114 | Accuracy:  0.805 | Val_Loss:  1.140 | Accuracy:  0.803\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch_optimizer as optim\n",
    "from torch.optim import SGD\n",
    "def train_loop(model, df_train, df_val):\n",
    "\n",
    "    train_dataset = DataSequence(df_train)\n",
    "    val_dataset = DataSequence(df_val)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=BATCH_SIZE)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    optimizer = SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    best_acc = 0\n",
    "    best_loss = 1000\n",
    "\n",
    "    for epoch_num in range(EPOCHS):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for train_data, train_label in tqdm(train_dataloader):\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_data['attention_mask'].squeeze(1).to(device)\n",
    "            input_id = train_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss, logits = model(input_id, mask, train_label)\n",
    "\n",
    "            for i in range(logits.shape[0]):\n",
    "\n",
    "              logits_clean = logits[i][train_label[i] != -100]\n",
    "              label_clean = train_label[i][train_label[i] != -100]\n",
    "\n",
    "              predictions = logits_clean.argmax(dim=1)\n",
    "              acc = (predictions == label_clean).float().mean()\n",
    "              total_acc_train += acc\n",
    "              total_loss_train += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        for val_data, val_label in val_dataloader:\n",
    "\n",
    "            val_label = val_label.to(device)\n",
    "            mask = val_data['attention_mask'].squeeze(1).to(device)\n",
    "            input_id = val_data['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            loss, logits = model(input_id, mask, val_label)\n",
    "\n",
    "            for i in range(logits.shape[0]):\n",
    "\n",
    "              logits_clean = logits[i][val_label[i] != -100]\n",
    "              label_clean = val_label[i][val_label[i] != -100]\n",
    "\n",
    "              predictions = logits_clean.argmax(dim=1)\n",
    "              acc = (predictions == label_clean).float().mean()\n",
    "              total_acc_val += acc\n",
    "              total_loss_val += loss.item()\n",
    "\n",
    "        val_accuracy = total_acc_val / len(df_val)\n",
    "        val_loss = total_loss_val / len(df_val)\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train): .3f} | Accuracy: {total_acc_train / len(df_train): .3f} | Val_Loss: {total_loss_val / len(df_val): .3f} | Accuracy: {total_acc_val / len(df_val): .3f}')\n",
    "\n",
    "LEARNING_RATE = 5e-3\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "model = BertModel()\n",
    "train_loop(model, df_train, df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33a3c9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2080 SUPER'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0bf15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00117d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "workbook = xlsxwriter.Workbook('xlmr_result.xlsx')\n",
    "worksheet = workbook.add_worksheet(\"My sheet\")\n",
    "df = pd.read_csv('/home/crimex/CRIMEX/Dataset/XLMR_train2.csv')\n",
    "df = df['text']\n",
    "df = df.values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b762ed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align_word_ids(texts):\n",
    "  \n",
    "    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)\n",
    "    label_all_tokens = True\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "\n",
    "        elif word_idx != previous_word_idx:\n",
    "            try:\n",
    "                label_ids.append(1)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        else:\n",
    "            try:\n",
    "                label_ids.append(1 if label_all_tokens else -100)\n",
    "            except:\n",
    "                label_ids.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    return label_ids\n",
    "\n",
    "def listToString(s):\n",
    "   \n",
    "    # initialize an empty string\n",
    "    str1 = \" \"\n",
    "   \n",
    "    # return string \n",
    "    return (str1.join(s))\n",
    "def evaluate_one_text(model, df):\n",
    "    workbook = xlsxwriter.Workbook('xlmr_result.xlsx')\n",
    "    worksheet = workbook.add_worksheet(\"My sheet\")\n",
    "    df = pd.read_csv('/home/crimex/CRIMEX/Dataset/sample_train7.csv')\n",
    "    df = df['text']\n",
    "    df = df.values.tolist()\n",
    "    row = 0\n",
    "    col = 1\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    for item in df :\n",
    "      text = tokenizer(item, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "      mask = text['attention_mask'].to(device)\n",
    "      input_id = text['input_ids'].to(device)\n",
    "      label_ids = torch.Tensor(align_word_ids(item)).unsqueeze(0).to(device)\n",
    "\n",
    "      logits = model(input_id, mask, None)\n",
    "      logits_clean = logits[0][label_ids != -100]\n",
    "\n",
    "      predictions = logits_clean.argmax(dim=1).tolist()\n",
    "      prediction_label = [ids_to_labels[i] for i in predictions]\n",
    "      str1 = ', '.join([str(elem) for elem in prediction_label])\n",
    "      text_tokenized = tokenizer(item, padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "      token=tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"][0])\n",
    "\n",
    "      str2 = ', '.join([str(elem) for elem in token])\n",
    "      worksheet.write(row, 2, str2)\n",
    "      worksheet.write(row, 0, item)\n",
    "      worksheet.write(row, col, str1)\n",
    "      row += 1\n",
    "    workbook.close()\n",
    "evaluate_one_text(model, df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeee6be",
   "metadata": {},
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa5c66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a99208",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# run libery location py file\n",
    "%run /home/crimex/CRIMEX/ner_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2dfcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ee3c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_eval import Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file\n",
    "y_test_read = pd.read_csv('/home/crimex/CRIMEX/Dataset/sample_train7.csv')\n",
    "# y_test = y_test_read['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26778cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_read#['tag'][0]\n",
    "# y_test_read['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738a8b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_read = pd.read_excel('/home/crimex/CRIMEX/xlmr_result.xlsx', header=None, names=['text', 'tag', 'split'])\n",
    "#,usecols='B', names=['tag']\n",
    "# y_pred = y_pred_read['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6383ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_read#['tag'][0]\n",
    "# y_pred_read#['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182c4704",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded790d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "y_pred = []\n",
    "count = 0\n",
    "for yti in y_test_read['tag']: \n",
    "    y_test.append(list(yti.split(\" \")))\n",
    "    y_pred.append(list(y_pred_read['tag'][count].split(\", \")))\n",
    "    count+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98169bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d76a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_type_eval = ['Criminal', 'Victim', 'Action', 'Location', 'Datetime',  'Item', \n",
    "                 'Rootcause', 'Trigger','worth', 'Enforcement']\n",
    "\n",
    "evaluator = Evaluator(y_test, y_pred, ent_type_eval)\n",
    "results, results_agg = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a52e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006ca341",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_agg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
